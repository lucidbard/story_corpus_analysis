{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Analysis - COMPLETE & SIMPLIFIED ‚ú®\n",
    "\n",
    "**RESTART INSTRUCTIONS:**\n",
    "1. **Restart the kernel** \n",
    "2. **Run cells 2-9 in order** (setup + test)\n",
    "3. **Then use cells 10-12 for full corpus processing & HTML output**\n",
    "\n",
    "## What this notebook does:\n",
    "- ‚úÖ **One simple processor** (no confusing multiple versions!)\n",
    "- ‚úÖ **Scene segmentation** that actually works\n",
    "- ‚úÖ **Full corpus processing** - analyze all books\n",
    "- ‚úÖ **HTML visualization output** - JSON data for dashboards\n",
    "- ‚úÖ **Clear, understandable code**\n",
    "- ‚úÖ **No hanging cells**\n",
    "\n",
    "## The cells:\n",
    "1. **Header** (this cell)\n",
    "2. **LLM Provider Setup** - Choose your LLM\n",
    "3. **Environment** - Load settings  \n",
    "4. **Connection** - Connect to LLM\n",
    "5. **Data Classes** - Scene and Goal definitions\n",
    "6. **Connection Test** - Make sure it works\n",
    "7. **Working Processor** - Basic processor (keep for compatibility)\n",
    "8. **‚ú® Simple Processor** - The new, clean solution!\n",
    "9. **üß™ Test** - Test the simple processor\n",
    "10. **üìö Corpus Processing** - Run on entire corpus\n",
    "11. **üé® HTML Visualization** - Format data for HTML dashboard\n",
    "12. **üìã Workflow Guide** - Complete usage examples\n",
    "13. **Footer**\n",
    "\n",
    "## Quick Workflow:\n",
    "1. **Setup**: Run cells 2-9 \n",
    "2. **Test**: See cell 9 results\n",
    "3. **Full corpus**: `corpus_results = process_entire_corpus()`\n",
    "4. **HTML data**: `viz_data, json_file = create_html_visualization(corpus_results)`\n",
    "\n",
    "**Ready to start fresh? Restart the kernel and run cells 2-9!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è LLM Provider Configuration\n",
      "Choose your preferred LLM provider and model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341c34883f64493db72f5a48b1cae022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(RadioButtons(description='LLM Provider:', index=2, options=('anthropic', 'openai', 'ollama'), v‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Remember to run the next cell after changing provider settings!\n"
     ]
    }
   ],
   "source": [
    "# LLM Provider Configuration\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create provider selection widget\n",
    "provider_widget = widgets.RadioButtons(\n",
    "    options=['anthropic', 'openai', 'ollama'],\n",
    "    value='ollama',\n",
    "    description='LLM Provider:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create model configuration widgets\n",
    "model_widgets = {\n",
    "    'anthropic': widgets.Dropdown(\n",
    "        options=['claude-opus-4-1-20250805', 'claude-sonnet-4-20250514', 'claude-3-5-haiku-latest','claude-3-5-sonnet-20241022', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229'],\n",
    "        value='claude-3-5-sonnet-20241022',\n",
    "        description='Anthropic Model:'\n",
    "    ),\n",
    "    'openai': widgets.Dropdown(\n",
    "        options=['gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo'],\n",
    "        value='gpt-4',\n",
    "        description='OpenAI Model:'\n",
    "    ),\n",
    "    'ollama': widgets.Dropdown(\n",
    "        options=['gpt-oss-32k', 'gpt-oss:latest', 'llama3:8b', 'mistral', 'codellama'],\n",
    "        value='gpt-oss-32k',\n",
    "        description='Ollama Model:'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Configuration display\n",
    "config_output = widgets.Output()\n",
    "\n",
    "def update_config(change=None):\n",
    "    with config_output:\n",
    "        config_output.clear_output(wait=True)\n",
    "        provider = provider_widget.value\n",
    "        model = model_widgets[provider].value\n",
    "        \n",
    "        print(f\"üîß Configuration:\")\n",
    "        print(f\"   Provider: {provider.upper()}\")\n",
    "        print(f\"   Model: {model}\")\n",
    "        \n",
    "        if provider == 'anthropic':\n",
    "            print(f\"   Context: 200k tokens | Max Output: 4k tokens\")\n",
    "            print(f\"   üìù Note: Requires ANTHROPIC_API_KEY environment variable\")\n",
    "        elif provider == 'openai':\n",
    "            print(f\"   Context: 128k tokens | Max Output: 4k tokens\")\n",
    "            print(f\"   üìù Note: Requires OPENAI_API_KEY environment variable\")\n",
    "        elif provider == 'ollama':\n",
    "            if model == 'gpt-oss-32k':\n",
    "                print(f\"   Context: 32k tokens | Max Output: 4k tokens\")\n",
    "                print(f\"   üìù Note: Custom model with enhanced context size\")\n",
    "            else:\n",
    "                print(f\"   Context: 8k tokens | Max Output: 1.5k tokens\")\n",
    "                print(f\"   üìù Note: Requires Ollama running locally with model downloaded\")\n",
    "\n",
    "# Setup widgets\n",
    "provider_widget.observe(update_config, names='value')\n",
    "for widget in model_widgets.values():\n",
    "    widget.observe(update_config, names='value')\n",
    "\n",
    "print(\"üéõÔ∏è LLM Provider Configuration\")\n",
    "print(\"Choose your preferred LLM provider and model:\")\n",
    "display(widgets.VBox([\n",
    "    provider_widget,\n",
    "    widgets.HBox([model_widgets['anthropic'], model_widgets['openai'], model_widgets['ollama']]),\n",
    "    config_output\n",
    "]))\n",
    "\n",
    "# Initialize display\n",
    "update_config()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Remember to run the next cell after changing provider settings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"üìù Falling back to system environment variables\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to load .env file: {e}\")\n",
    "    print(\"üìù Falling back to system environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Selected Provider: OLLAMA\n",
      "ü§ñ Selected Model: gpt-oss-32k\n",
      "‚úÖ Ollama connection established!\n",
      "‚úÖ Imports and OLLAMA LLM setup complete\n",
      "üìã Configuration: gpt-oss-32k | Max Tokens: 1500 | Context: 15192\n",
      "üöÄ Ready to process!\n",
      "‚úÖ Ollama connection established!\n",
      "‚úÖ Imports and OLLAMA LLM setup complete\n",
      "üìã Configuration: gpt-oss-32k | Max Tokens: 1500 | Context: 15192\n",
      "üöÄ Ready to process!\n"
     ]
    }
   ],
   "source": [
    "# Get LLM provider configuration from widgets (if available)\n",
    "try:\n",
    "    LLM_PROVIDER = provider_widget.value\n",
    "    if LLM_PROVIDER == 'anthropic':\n",
    "        MODEL_NAME = model_widgets['anthropic'].value\n",
    "    elif LLM_PROVIDER == 'openai':\n",
    "        MODEL_NAME = model_widgets['openai'].value\n",
    "    elif LLM_PROVIDER == 'ollama':\n",
    "        MODEL_NAME = model_widgets['ollama'].value\n",
    "except NameError:\n",
    "    # Fallback if widgets not defined\n",
    "    LLM_PROVIDER = \"ollama\"\n",
    "    MODEL_NAME = \"gpt-oss-32k\"\n",
    "\n",
    "print(f\"üéØ Selected Provider: {LLM_PROVIDER.upper()}\")\n",
    "print(f\"ü§ñ Selected Model: {MODEL_NAME}\")\n",
    "\n",
    "# Initialize LLM clients based on provider selection\n",
    "anthropic_client = None\n",
    "openai_client = None\n",
    "ollama_client = None\n",
    "\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    try:\n",
    "        import anthropic\n",
    "        api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "        if not api_key:\n",
    "            print(\"‚ö†Ô∏è Warning: ANTHROPIC_API_KEY not found in environment variables\")\n",
    "            print(\"üìù Make sure your .env file contains: ANTHROPIC_API_KEY='your_key_here'\")\n",
    "        anthropic_client = anthropic.Anthropic(api_key=api_key) if api_key else None\n",
    "        print(\"‚úÖ Anthropic client initialized!\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå anthropic package not installed. Run: pip install anthropic\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize Anthropic: {e}\")\n",
    "\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    try:\n",
    "        import openai\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not found in environment variables\")\n",
    "            print(\"üìù Make sure your .env file contains: OPENAI_API_KEY='your_key_here'\")\n",
    "        openai_client = openai.OpenAI(api_key=api_key) if api_key else None\n",
    "        print(\"‚úÖ OpenAI client initialized!\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå openai package not installed. Run: pip install openai\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize OpenAI: {e}\")\n",
    "\n",
    "elif LLM_PROVIDER == \"ollama\":\n",
    "    try:\n",
    "        import ollama\n",
    "        # Set up working Ollama client for Windows/WSL\n",
    "        ollama_client = ollama.Client(host='http://172.21.144.1:11434')\n",
    "        \n",
    "        # Test connection with a simple request\n",
    "        test_response = ollama_client.generate(\n",
    "            model=MODEL_NAME,\n",
    "            prompt='Test connection',\n",
    "            options={\n",
    "                'num_predict': 10,\n",
    "                'temperature': 0.1\n",
    "            }\n",
    "        )\n",
    "        print(\"‚úÖ Ollama connection established!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå ollama package not installed. Run: pip install ollama\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to Ollama: {e}\")\n",
    "        print(\"Please ensure Ollama is running with the selected model\")\n",
    "        ollama_client = None\n",
    "\n",
    "# Configuration based on provider\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    MAX_TOKENS = 4000\n",
    "    CONTEXT_SIZE = 200000\n",
    "    TEMPERATURE = 0.1\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    MAX_TOKENS = 4000\n",
    "    CONTEXT_SIZE = 128000\n",
    "    TEMPERATURE = 0.1\n",
    "elif LLM_PROVIDER == \"ollama\":\n",
    "    MAX_TOKENS = 1500\n",
    "    CONTEXT_SIZE = 15192\n",
    "    TEMPERATURE = 0.1\n",
    "\n",
    "# Data directories\n",
    "DATA_DIR = Path('/home/lucid/story_analysis/corpus_clean/clean corpus no paratext')\n",
    "RESULTS_DIR = Path('/home/lucid/story_analysis/results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Imports and {LLM_PROVIDER.upper()} LLM setup complete\")\n",
    "print(f\"üìã Configuration: {MODEL_NAME} | Max Tokens: {MAX_TOKENS} | Context: {CONTEXT_SIZE}\")\n",
    "\n",
    "# Verify client is ready\n",
    "client_ready = False\n",
    "if LLM_PROVIDER == \"anthropic\" and anthropic_client:\n",
    "    client_ready = True\n",
    "elif LLM_PROVIDER == \"openai\" and openai_client:\n",
    "    client_ready = True\n",
    "elif LLM_PROVIDER == \"ollama\" and ollama_client:\n",
    "    client_ready = True\n",
    "\n",
    "if client_ready:\n",
    "    print(\"üöÄ Ready to process!\")\n",
    "else:\n",
    "    print(\"‚ùå Client not ready. Please check configuration and credentials.\")\n",
    "    print(\"üí° For API providers, ensure your .env file contains the correct API keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data structures defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Scene:\n",
    "    \"\"\"Represents a scene within a chapter\"\"\"\n",
    "    scene_id: str  # Format: \"book_X_chapter_Y_scene_Z\"\n",
    "    book_id: str\n",
    "    chapter_num: int\n",
    "    scene_num: int\n",
    "    text: str\n",
    "    narrator: Optional[str] = None\n",
    "    start_paragraph: Optional[int] = None\n",
    "    end_paragraph: Optional[int] = None\n",
    "\n",
    "@dataclass\n",
    "class Goal:\n",
    "    \"\"\"Represents a character goal with dual categorization\"\"\"\n",
    "    goal_id: str\n",
    "    scene_id: str\n",
    "    character: str\n",
    "    goal_text: str\n",
    "    motivation_type: str  # \"internal\" or \"external\"\n",
    "    category: str  # \"academic\", \"family\", \"personal\", \"social\", \"work\", etc.\n",
    "    evidence: str  # Text evidence supporting this goal\n",
    "    confidence: float  # LLM confidence score 0-1\n",
    "\n",
    "@dataclass\n",
    "class ProcessingProgress:\n",
    "    \"\"\"Tracks processing progress for resumable operations\"\"\"\n",
    "    books_segmented: List[str]\n",
    "    books_narrators_identified: List[str]\n",
    "    books_goals_analyzed: List[str]\n",
    "    last_updated: str\n",
    "    total_books: int\n",
    "    \n",
    "    def save(self, filepath: Path):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(asdict(self), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: Path):\n",
    "        if filepath.exists():\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            return cls(**data)\n",
    "        return cls([], [], [], datetime.now().isoformat(), 0)\n",
    "\n",
    "print(\"‚úÖ Data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LLM Provider Status:\n",
      "   Current: OLLAMA (gpt-oss-32k)\n",
      "   Context: 15,192 tokens | Max Output: 1,500 tokens\n",
      "   Anthropic: ‚ùå\n",
      "   OpenAI: ‚ùå\n",
      "   Ollama: ‚úÖ\n",
      "\n",
      "üîç Getting gpt-oss-32k model information...\n",
      "üìä Detected context size: 32,768 tokens\n",
      "\n",
      "üîß Testing basic connection...\n",
      "üß™ Testing OLLAMA connection...\n",
      "‚úÖ Ollama response: {'test': 'success'}\n",
      "\n",
      "üí° Quick Commands:\n",
      "   show_provider_status() - Show all provider statuses\n",
      "   test_llm_connection() - Test current provider\n",
      "   test_ollama_context_systematically() - Systematic Ollama context testing\n",
      "   update_context_settings() - Auto-optimize context settings\n",
      "   configure_ollama_context(size) - Set specific Ollama context size\n",
      "   quick_switch_provider('provider', 'model') - Switch providers\n",
      "‚úÖ Ollama response: {'test': 'success'}\n",
      "\n",
      "üí° Quick Commands:\n",
      "   show_provider_status() - Show all provider statuses\n",
      "   test_llm_connection() - Test current provider\n",
      "   test_ollama_context_systematically() - Systematic Ollama context testing\n",
      "   update_context_settings() - Auto-optimize context settings\n",
      "   configure_ollama_context(size) - Set specific Ollama context size\n",
      "   quick_switch_provider('provider', 'model') - Switch providers\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for provider management and dynamic context sizing\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model_name: str = None) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken for OpenAI models or estimate for others\"\"\"\n",
    "    try:\n",
    "        if LLM_PROVIDER == \"openai\" and model_name:\n",
    "            encoding = tiktoken.encoding_for_model(model_name)\n",
    "            return len(encoding.encode(text))\n",
    "        else:\n",
    "            # Rough estimation: ~4 characters per token\n",
    "            return len(text) // 4\n",
    "    except Exception:\n",
    "        # Fallback estimation\n",
    "        return len(text) // 4\n",
    "\n",
    "def get_ollama_model_info(model_name: str) -> dict:\n",
    "    \"\"\"Get model information from Ollama including context size\"\"\"\n",
    "    try:\n",
    "        if ollama_client:\n",
    "            # Try to get model info\n",
    "            response = ollama_client.show(model_name)\n",
    "            \n",
    "            # Extract context size from model info\n",
    "            context_size = 2048  # Default fallback\n",
    "            \n",
    "            # Look for context size in various places\n",
    "            if 'modelinfo' in response:\n",
    "                modelinfo = response['modelinfo']\n",
    "                # Common parameter names for context size\n",
    "                for param in ['num_ctx', 'context_length', 'n_ctx', 'max_context']:\n",
    "                    if param in modelinfo:\n",
    "                        context_size = int(modelinfo[param])\n",
    "                        break\n",
    "            \n",
    "            # Handle our custom models and known models\n",
    "            if 'gpt-oss-32k' in model_name.lower():\n",
    "                # Our custom model with 32k context\n",
    "                context_size = 32768\n",
    "            elif 'gpt-oss' in model_name.lower():\n",
    "                # Original gpt-oss model\n",
    "                context_size = max(context_size, 32768)  # At least 32k\n",
    "            elif 'llama3:8b' in model_name.lower():\n",
    "                context_size = max(context_size, 8192)\n",
    "            elif 'llama2' in model_name.lower():\n",
    "                context_size = max(context_size, 4096)\n",
    "            elif 'mistral' in model_name.lower():\n",
    "                context_size = max(context_size, 32768)\n",
    "            elif 'codellama' in model_name.lower():\n",
    "                context_size = max(context_size, 16384)\n",
    "            \n",
    "            return {\n",
    "                'context_size': context_size,\n",
    "                'model_info': response.get('modelinfo', {}),\n",
    "                'parameters': response.get('parameters', {})\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not get model info: {e}\")\n",
    "    \n",
    "    # Fallback defaults\n",
    "    if 'gpt-oss-32k' in model_name.lower():\n",
    "        return {'context_size': 32768, 'model_info': {}, 'parameters': {}}\n",
    "    elif 'gpt-oss' in model_name.lower():\n",
    "        return {'context_size': 32768, 'model_info': {}, 'parameters': {}}\n",
    "    elif 'llama3:8b' in model_name.lower():\n",
    "        return {'context_size': 8192, 'model_info': {}, 'parameters': {}}\n",
    "    elif 'llama2' in model_name.lower():\n",
    "        return {'context_size': 4096, 'model_info': {}, 'parameters': {}}\n",
    "    elif 'mistral' in model_name.lower():\n",
    "        return {'context_size': 32768, 'model_info': {}, 'parameters': {}}\n",
    "    elif 'codellama' in model_name.lower():\n",
    "        return {'context_size': 16384, 'model_info': {}, 'parameters': {}}\n",
    "    else:\n",
    "        return {'context_size': 8192, 'model_info': {}, 'parameters': {}}\n",
    "\n",
    "def get_optimal_context_size(provider: str, model: str) -> dict:\n",
    "    \"\"\"Get optimal context sizes for different models\"\"\"\n",
    "    if provider == 'ollama':\n",
    "        # Get actual model info for Ollama\n",
    "        model_info = get_ollama_model_info(model)\n",
    "        max_context = model_info['context_size']\n",
    "        print(f\"üîç Detected Ollama model context size: {max_context:,} tokens\")\n",
    "    else:\n",
    "        # Static limits for other providers\n",
    "        context_limits = {\n",
    "            'anthropic': {\n",
    "                'claude-3-5-sonnet-20241022': 200000,\n",
    "                'claude-3-opus-20240229': 200000,\n",
    "                'claude-3-sonnet-20240229': 200000,\n",
    "                'default': 200000\n",
    "            },\n",
    "            'openai': {\n",
    "                'gpt-4': 128000,\n",
    "                'gpt-4-turbo': 128000,\n",
    "                'gpt-4o': 128000,\n",
    "                'gpt-3.5-turbo': 16385,\n",
    "                'default': 128000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        provider_limits = context_limits.get(provider, {})\n",
    "        max_context = provider_limits.get(model, provider_limits.get('default', 8192))\n",
    "    \n",
    "    # Reserve 20% for output and system overhead\n",
    "    usable_context = int(max_context * 0.8)\n",
    "    max_output = min(4000, int(max_context * 0.2))\n",
    "    \n",
    "    return {\n",
    "        'max_context': max_context,\n",
    "        'usable_context': usable_context,\n",
    "        'max_output': max_output\n",
    "    }\n",
    "\n",
    "def chunk_text_for_context(text: str, max_tokens: int, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into chunks that fit within context limits with overlap\"\"\"\n",
    "    if count_tokens(text, MODEL_NAME) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    # Split by paragraphs first\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_tokens = count_tokens(para, MODEL_NAME)\n",
    "        \n",
    "        # If single paragraph is too large, split it further\n",
    "        if para_tokens > max_tokens:\n",
    "            # Split by sentences\n",
    "            sentences = para.split('. ')\n",
    "            for sentence in sentences:\n",
    "                sentence_tokens = count_tokens(sentence, MODEL_NAME)\n",
    "                if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    # Keep overlap\n",
    "                    overlap_text = '. '.join(current_chunk.split('. ')[-overlap//20:])\n",
    "                    current_chunk = overlap_text + \". \" + sentence\n",
    "                    current_tokens = count_tokens(current_chunk, MODEL_NAME)\n",
    "                else:\n",
    "                    current_chunk += \". \" + sentence if current_chunk else sentence\n",
    "                    current_tokens += sentence_tokens\n",
    "        else:\n",
    "            # Check if adding this paragraph exceeds limit\n",
    "            if current_tokens + para_tokens > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                # Keep overlap\n",
    "                overlap_paras = current_chunk.split('\\n\\n')[-2:] if len(current_chunk.split('\\n\\n')) > 1 else []\n",
    "                current_chunk = '\\n\\n'.join(overlap_paras) + '\\n\\n' + para if overlap_paras else para\n",
    "                current_tokens = count_tokens(current_chunk, MODEL_NAME)\n",
    "            else:\n",
    "                current_chunk += '\\n\\n' + para if current_chunk else para\n",
    "                current_tokens += para_tokens\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def test_ollama_context_systematically():\n",
    "    \"\"\"Test the Ollama model with realistic context sizes for the new gpt-oss-32k model\"\"\"\n",
    "    if LLM_PROVIDER != \"ollama\" or not ollama_client:\n",
    "        print(\"‚ùå This function only works with Ollama\")\n",
    "        return\n",
    "        \n",
    "    print(f\"üî¨ Systematic context testing for {MODEL_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get model info first\n",
    "    model_info = get_ollama_model_info(MODEL_NAME)\n",
    "    print(f\"üìä Model claims context size: {model_info['context_size']:,} tokens\")\n",
    "    \n",
    "    # Test with realistic incremental sizes\n",
    "    if 'gpt-oss-32k' in MODEL_NAME:\n",
    "        test_contexts = [4096, 8192, 16384, 24576, 32768]\n",
    "    else:\n",
    "        test_contexts = [2048, 4096, 8192, 16384]\n",
    "    \n",
    "    working_context = 2048  # Conservative default\n",
    "    max_successful_tokens = 0\n",
    "    \n",
    "    for ctx_size in test_contexts:\n",
    "        print(f\"\\nüß™ Testing with num_ctx={ctx_size:,}...\")\n",
    "        \n",
    "        # Create a test that uses about 60% of the context for safety\n",
    "        target_tokens = int(ctx_size * 0.6)\n",
    "        \n",
    "        # Create story-like content similar to what we'll actually process\n",
    "        base_story = \"\"\"\n",
    "        Kristy Thomas sat in her room thinking about the Baby-Sitters Club meeting that was about to start. \n",
    "        As club president, she always made sure everything was organized and running smoothly. The other members \n",
    "        would arrive soon: Claudia Kishi with her artistic flair, Mary Anne Spier with her careful scheduling, \n",
    "        and Stacey McGill with her math skills for handling the money.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Repeat the story to reach target tokens\n",
    "        repetitions = max(1, target_tokens // count_tokens(base_story, MODEL_NAME))\n",
    "        test_text = base_story * repetitions\n",
    "        actual_tokens = count_tokens(test_text, MODEL_NAME)\n",
    "        \n",
    "        test_prompt = f\"Please analyze the main themes in this text and respond in 2-3 sentences:\\n\\n{test_text}\"\n",
    "        prompt_tokens = count_tokens(test_prompt, MODEL_NAME)\n",
    "        \n",
    "        print(f\"  üìù Prompt tokens: {prompt_tokens:,}\")\n",
    "        print(f\"  üéØ Target context: {ctx_size:,}\")\n",
    "        \n",
    "        try:\n",
    "            response = ollama_client.generate(\n",
    "                model=MODEL_NAME,\n",
    "                prompt=test_prompt,\n",
    "                options={\n",
    "                    'num_predict': 200,\n",
    "                    'temperature': 0.3,\n",
    "                    'num_ctx': ctx_size,\n",
    "                    'top_p': 0.9\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if response and response.get('response') and len(response['response'].strip()) > 20:\n",
    "                working_context = ctx_size\n",
    "                max_successful_tokens = prompt_tokens\n",
    "                print(f\"‚úÖ Success with num_ctx={ctx_size:,} (input: {prompt_tokens:,} tokens)\")\n",
    "                print(f\"   Response preview: {response['response'][:150]}...\")\n",
    "            else:\n",
    "                print(f\"‚ùå Empty/minimal response with num_ctx={ctx_size:,}\")\n",
    "                print(f\"   Response: '{response.get('response', 'None')[:50]}...'\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with num_ctx={ctx_size:,}: {str(e)[:100]}...\")\n",
    "            # If we get a memory error, stop testing larger sizes\n",
    "            if any(word in str(e).lower() for word in [\"memory\", \"cuda\", \"out of memory\", \"allocation\"]):\n",
    "                print(\"   üíæ Memory limitation reached\")\n",
    "                break\n",
    "            # For other errors, continue testing\n",
    "            continue\n",
    "            \n",
    "        time.sleep(2)  # Give GPU time to clear memory between tests\n",
    "    \n",
    "    print(f\"\\nüéØ Test Results:\")\n",
    "    print(f\"   Maximum working context: {working_context:,} tokens\")\n",
    "    print(f\"   Maximum successful input: {max_successful_tokens:,} tokens\")\n",
    "    print(f\"   Recommended working size: {int(working_context * 0.8):,} tokens\")\n",
    "    \n",
    "    # Update global setting\n",
    "    global CONTEXT_SIZE\n",
    "    old_size = CONTEXT_SIZE\n",
    "    CONTEXT_SIZE = int(working_context * 0.8)  # Use 80% for safety\n",
    "    \n",
    "    print(f\"\\nüîß Updated CONTEXT_SIZE: {old_size:,} ‚Üí {CONTEXT_SIZE:,} tokens\")\n",
    "    \n",
    "    # Calculate what this means for story analysis tasks\n",
    "    print(f\"\\nüìö Story Analysis Capacity:\")\n",
    "    print(f\"   Narrator identification: ‚úÖ (needs ~500-1,500 tokens)\")\n",
    "    print(f\"   Scene segmentation: {'‚úÖ' if CONTEXT_SIZE >= 4000 else '‚ö†Ô∏è'} (needs ~2,000-8,000 tokens)\")\n",
    "    print(f\"   Goal analysis: {'‚úÖ' if CONTEXT_SIZE >= 2000 else '‚ö†Ô∏è'} (needs ~1,000-4,000 tokens)\")\n",
    "    print(f\"   Large chapters: {'‚úÖ' if CONTEXT_SIZE >= 15000 else '‚ö†Ô∏è'} (some chapters are 10k+ tokens)\")\n",
    "    \n",
    "    return working_context\n",
    "\n",
    "def test_context_limits():\n",
    "    \"\"\"Test and determine optimal context size for current provider\"\"\"\n",
    "    print(f\"üß™ Testing context limits for {LLM_PROVIDER.upper()} ({MODEL_NAME})...\")\n",
    "    \n",
    "    # Get theoretical limits\n",
    "    limits = get_optimal_context_size(LLM_PROVIDER, MODEL_NAME)\n",
    "    print(f\"üìä Theoretical limits:\")\n",
    "    print(f\"   Max context: {limits['max_context']:,} tokens\")\n",
    "    print(f\"   Usable context: {limits['usable_context']:,} tokens\")\n",
    "    print(f\"   Max output: {limits['max_output']:,} tokens\")\n",
    "    \n",
    "    # For Ollama, also show model-specific info\n",
    "    if LLM_PROVIDER == \"ollama\":\n",
    "        model_info = get_ollama_model_info(MODEL_NAME)\n",
    "        if model_info.get('parameters'):\n",
    "            print(f\"üìã Model parameters: {model_info['parameters']}\")\n",
    "    \n",
    "    # Test with progressively larger inputs\n",
    "    test_sizes = [1000, 5000, 10000, 20000]\n",
    "    \n",
    "    # Adjust test sizes based on detected limits\n",
    "    if limits['usable_context'] > 20000:\n",
    "        test_sizes.extend([50000, 100000])\n",
    "    if limits['usable_context'] > 100000:\n",
    "        test_sizes.extend([150000, 200000])\n",
    "    \n",
    "    working_size = 1000\n",
    "    \n",
    "    for size in test_sizes:\n",
    "        if size > limits['usable_context']:\n",
    "            break\n",
    "            \n",
    "        # Create test text\n",
    "        test_text = \"Test sentence. \" * (size // 15)  # Approximate tokens\n",
    "        actual_tokens = count_tokens(test_text, MODEL_NAME)\n",
    "        \n",
    "        print(f\"\\nüîç Testing {actual_tokens:,} tokens...\")\n",
    "        \n",
    "        try:\n",
    "            if LLM_PROVIDER == \"anthropic\" and anthropic_client:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    max_tokens=100,\n",
    "                    temperature=0,\n",
    "                    messages=[{\"role\": \"user\", \"content\": f\"Summarize this in one word: {test_text}\"}]\n",
    "                )\n",
    "                if response.content[0].text.strip():\n",
    "                    working_size = actual_tokens\n",
    "                    print(f\"‚úÖ Success at {actual_tokens:,} tokens\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Empty response at {actual_tokens:,} tokens\")\n",
    "                    break\n",
    "                    \n",
    "            elif LLM_PROVIDER == \"openai\" and openai_client:\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    max_tokens=100,\n",
    "                    temperature=0,\n",
    "                    messages=[{\"role\": \"user\", \"content\": f\"Summarize this in one word: {test_text}\"}]\n",
    "                )\n",
    "                if response.choices[0].message.content and response.choices[0].message.content.strip():\n",
    "                    working_size = actual_tokens\n",
    "                    print(f\"‚úÖ Success at {actual_tokens:,} tokens\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Empty response at {actual_tokens:,} tokens\")\n",
    "                    break\n",
    "                    \n",
    "            elif LLM_PROVIDER == \"ollama\" and ollama_client:\n",
    "                # For Ollama, use the working parameters we discovered\n",
    "                required_context = actual_tokens + 500  # Add buffer for output\n",
    "                context_to_use = min(required_context, limits['max_context'])\n",
    "                \n",
    "                print(f\"  üîß Setting num_ctx to {context_to_use:,} tokens\")\n",
    "                \n",
    "                response = ollama_client.generate(\n",
    "                    model=MODEL_NAME,\n",
    "                    prompt=f\"Summarize this in one word: {test_text}\",\n",
    "                    options={\n",
    "                        'num_predict': 100, \n",
    "                        'temperature': 0.3,\n",
    "                        'num_ctx': context_to_use,\n",
    "                        'top_p': 0.9\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if response.get('response', '').strip():\n",
    "                    working_size = actual_tokens\n",
    "                    print(f\"‚úÖ Success at {actual_tokens:,} tokens\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Empty response at {actual_tokens:,} tokens\")\n",
    "                    break\n",
    "            \n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed at {actual_tokens:,} tokens: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüéØ Recommended working context size: {working_size:,} tokens\")\n",
    "    return working_size\n",
    "\n",
    "def update_context_settings():\n",
    "    \"\"\"Update global context settings based on testing\"\"\"\n",
    "    global CONTEXT_SIZE, MAX_TOKENS\n",
    "    \n",
    "    # Use the systematic testing for Ollama\n",
    "    if LLM_PROVIDER == \"ollama\":\n",
    "        optimal_size = test_ollama_context_systematically()\n",
    "    else:\n",
    "        optimal_size = test_context_limits()\n",
    "        \n",
    "    limits = get_optimal_context_size(LLM_PROVIDER, MODEL_NAME)\n",
    "    \n",
    "    # Use conservative size (80% of tested working size)\n",
    "    CONTEXT_SIZE = int(optimal_size * 0.8)\n",
    "    MAX_TOKENS = min(limits['max_output'], 4000)\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Final Updated Settings:\")\n",
    "    print(f\"   Context size: {CONTEXT_SIZE:,} tokens\")\n",
    "    print(f\"   Max output: {MAX_TOKENS:,} tokens\")\n",
    "    \n",
    "    return CONTEXT_SIZE, MAX_TOKENS\n",
    "\n",
    "def configure_ollama_context(context_size: int = None):\n",
    "    \"\"\"Configure Ollama with a specific context size\"\"\"\n",
    "    if LLM_PROVIDER != \"ollama\" or not ollama_client:\n",
    "        print(\"‚ùå This function only works with Ollama\")\n",
    "        return\n",
    "    \n",
    "    if not context_size:\n",
    "        # Auto-detect optimal size\n",
    "        limits = get_optimal_context_size(LLM_PROVIDER, MODEL_NAME)\n",
    "        context_size = limits['usable_context']\n",
    "    \n",
    "    global CONTEXT_SIZE\n",
    "    CONTEXT_SIZE = context_size\n",
    "    \n",
    "    print(f\"üîß Configured Ollama context size: {CONTEXT_SIZE:,} tokens\")\n",
    "    print(f\"üí° This will be used in the num_ctx parameter for requests\")\n",
    "\n",
    "def test_llm_connection():\n",
    "    \"\"\"Test connection to the currently selected LLM provider\"\"\"\n",
    "    print(f\"üß™ Testing {LLM_PROVIDER.upper()} connection...\")\n",
    "    \n",
    "    test_prompt = \"Respond with exactly: {'test': 'success'}\"\n",
    "    \n",
    "    try:\n",
    "        if LLM_PROVIDER == \"anthropic\" and anthropic_client:\n",
    "            response = anthropic_client.messages.create(\n",
    "                model=MODEL_NAME,\n",
    "                max_tokens=100,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": test_prompt}]\n",
    "            )\n",
    "            print(f\"‚úÖ Anthropic response: {response.content[0].text}\")\n",
    "            \n",
    "        elif LLM_PROVIDER == \"openai\" and openai_client:\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                max_tokens=100,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": test_prompt}]\n",
    "            )\n",
    "            print(f\"‚úÖ OpenAI response: {response.choices[0].message.content}\")\n",
    "            \n",
    "        elif LLM_PROVIDER == \"ollama\" and ollama_client:\n",
    "            response = ollama_client.generate(\n",
    "                model=MODEL_NAME,\n",
    "                prompt=test_prompt,\n",
    "                options={\n",
    "                    'num_predict': 100, \n",
    "                    'temperature': 0.3, \n",
    "                    'num_ctx': CONTEXT_SIZE,\n",
    "                    'top_p': 0.9\n",
    "                }\n",
    "            )\n",
    "            print(f\"‚úÖ Ollama response: {response['response']}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå No valid {LLM_PROVIDER} client available\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def show_provider_status():\n",
    "    \"\"\"Show the status of all LLM providers\"\"\"\n",
    "    print(\"üìä LLM Provider Status:\")\n",
    "    print(f\"   Current: {LLM_PROVIDER.upper()} ({MODEL_NAME})\")\n",
    "    print(f\"   Context: {CONTEXT_SIZE:,} tokens | Max Output: {MAX_TOKENS:,} tokens\")\n",
    "    print(f\"   Anthropic: {'‚úÖ' if anthropic_client else '‚ùå'}\")\n",
    "    print(f\"   OpenAI: {'‚úÖ' if openai_client else '‚ùå'}\")\n",
    "    print(f\"   Ollama: {'‚úÖ' if ollama_client else '‚ùå'}\")\n",
    "\n",
    "def quick_switch_provider(provider: str, model: str = None):\n",
    "    \"\"\"Quick function to switch providers programmatically\"\"\"\n",
    "    global LLM_PROVIDER, MODEL_NAME\n",
    "    \n",
    "    if provider.lower() not in ['anthropic', 'openai', 'ollama']:\n",
    "        print(\"‚ùå Invalid provider. Choose: anthropic, openai, or ollama\")\n",
    "        return\n",
    "    \n",
    "    # Update widget values\n",
    "    provider_widget.value = provider.lower()\n",
    "    \n",
    "    if model:\n",
    "        if provider.lower() in model_widgets:\n",
    "            model_widgets[provider.lower()].value = model\n",
    "    \n",
    "    print(f\"üîÑ Switched to {provider.upper()}\")\n",
    "    print(\"üí° Run the imports cell again to initialize the new provider\")\n",
    "\n",
    "# Test current connection and get model info\n",
    "show_provider_status()\n",
    "\n",
    "if LLM_PROVIDER == \"ollama\" and ollama_client:\n",
    "    print(f\"\\nüîç Getting {MODEL_NAME} model information...\")\n",
    "    model_info = get_ollama_model_info(MODEL_NAME)\n",
    "    print(f\"üìä Detected context size: {model_info['context_size']:,} tokens\")\n",
    "\n",
    "if any([anthropic_client, openai_client, ollama_client]):\n",
    "    print(\"\\nüîß Testing basic connection...\")\n",
    "    test_llm_connection()\n",
    "\n",
    "print(\"\\nüí° Quick Commands:\")\n",
    "print(\"   show_provider_status() - Show all provider statuses\")\n",
    "print(\"   test_llm_connection() - Test current provider\")\n",
    "print(\"   test_ollama_context_systematically() - Systematic Ollama context testing\")\n",
    "print(\"   update_context_settings() - Auto-optimize context settings\")\n",
    "print(\"   configure_ollama_context(size) - Set specific Ollama context size\")\n",
    "print(\"   quick_switch_provider('provider', 'model') - Switch providers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WorkingStoryProcessor class defined with complete LLM integration\n"
     ]
    }
   ],
   "source": [
    "# Robust JSON parsing function\n",
    "def robust_json_parse(response_text: str):\n",
    "    \"\"\"Parse JSON from LLM response with multiple fallback strategies\"\"\"\n",
    "    if not response_text:\n",
    "        return None\n",
    "    \n",
    "    # Strategy 1: Try to parse as-is\n",
    "    try:\n",
    "        return json.loads(response_text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Look for JSON block\n",
    "    import re\n",
    "    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Strategy 3: Look for array\n",
    "    array_match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n",
    "    if array_match:\n",
    "        try:\n",
    "            return json.loads(array_match.group())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Could not parse JSON from: {response_text[:100]}...\")\n",
    "    return None\n",
    "\n",
    "class WorkingStoryProcessor:\n",
    "    \"\"\"A properly working story processor with correct token settings\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, results_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Conservative chunk sizes that work reliably\n",
    "        self.max_scene_chunk_chars = 3000\n",
    "        self.max_goal_chunk_chars = 4000\n",
    "    \n",
    "    def call_llm(self, prompt: str, max_retries: int = 3) -> Optional[Dict]:\n",
    "        \"\"\"Call LLM API with unified interface for all providers\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response_text = None\n",
    "                \n",
    "                if LLM_PROVIDER == \"anthropic\" and anthropic_client:\n",
    "                    response = anthropic_client.messages.create(\n",
    "                        model=MODEL_NAME,\n",
    "                        max_tokens=MAX_TOKENS,\n",
    "                        temperature=TEMPERATURE,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    response_text = response.content[0].text\n",
    "                \n",
    "                elif LLM_PROVIDER == \"openai\" and openai_client:\n",
    "                    response = openai_client.chat.completions.create(\n",
    "                        model=MODEL_NAME,\n",
    "                        max_tokens=MAX_TOKENS,\n",
    "                        temperature=TEMPERATURE,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    response_text = response.choices[0].message.content\n",
    "                \n",
    "                elif LLM_PROVIDER == \"ollama\" and ollama_client:\n",
    "                    # Dynamic context sizing for Ollama\n",
    "                    prompt_tokens = count_tokens(prompt, MODEL_NAME)\n",
    "                    dynamic_context = min(prompt_tokens + MAX_TOKENS + 500, CONTEXT_SIZE)\n",
    "                    \n",
    "                    response = ollama_client.generate(\n",
    "                        model=MODEL_NAME,\n",
    "                        prompt=prompt,\n",
    "                        options={\n",
    "                            'num_predict': MAX_TOKENS,\n",
    "                            'temperature': TEMPERATURE,\n",
    "                            'num_ctx': dynamic_context\n",
    "                        }\n",
    "                    )\n",
    "                    response_text = response['response']\n",
    "                \n",
    "                else:\n",
    "                    raise Exception(f\"No valid {LLM_PROVIDER} client available\")\n",
    "                \n",
    "                if not response_text:\n",
    "                    raise Exception(\"Empty response from LLM\")\n",
    "                \n",
    "                # Parse JSON response\n",
    "                return robust_json_parse(response_text)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"Failed to get LLM response after {max_retries} attempts\")\n",
    "                    return None\n",
    "    \n",
    "    def segment_scenes(self, book_id: str, text: str):\n",
    "        \"\"\"Segment scenes with proper token settings\"\"\"\n",
    "        \n",
    "        if len(text) <= self.max_scene_chunk_chars:\n",
    "            return self._segment_single_chunk(book_id, text)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self._split_text(text, self.max_scene_chunk_chars)\n",
    "        all_scenes = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"   üìÑ Processing scene chunk {i+1}/{len(chunks)} ({len(chunk):,} chars)\")\n",
    "            scenes = self._segment_single_chunk(book_id, chunk, scene_offset=len(all_scenes))\n",
    "            all_scenes.extend(scenes)\n",
    "            print(f\"      Found {len(scenes)} scenes\")\n",
    "        \n",
    "        return all_scenes\n",
    "    \n",
    "    def _segment_single_chunk(self, book_id: str, text: str, scene_offset: int = 0):\n",
    "        \"\"\"Segment scenes in a single chunk with proper settings\"\"\"\n",
    "        \n",
    "        prompt = f'''Identify natural scene breaks in this Baby-sitter Club book text.\n",
    "\n",
    "A scene is a continuous sequence in the same location/time. Look for:\n",
    "- Location changes\n",
    "- Time jumps  \n",
    "- Clear narrative breaks\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Respond in JSON:\n",
    "{{\n",
    "  \"scenes\": [\n",
    "    {{\n",
    "      \"scene_num\": 1,\n",
    "      \"description\": \"Brief scene description\"\n",
    "    }}\n",
    "  ]\n",
    "}}'''\n",
    "        \n",
    "        try:\n",
    "            result = self.call_llm(prompt)\n",
    "            if result and 'scenes' in result:\n",
    "                scenes = result['scenes']\n",
    "                # Add metadata and adjust numbers\n",
    "                for scene in scenes:\n",
    "                    scene['scene_num'] += scene_offset\n",
    "                    scene['book_id'] = book_id\n",
    "                    scene['text'] = text  # Store the chunk text\n",
    "                    scene['scene_id'] = f\"{book_id}_scene_{scene['scene_num']}\"\n",
    "                return scenes\n",
    "        except Exception as e:\n",
    "            print(f\"         Error in scene segmentation: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def analyze_goals(self, scene_data):\n",
    "        \"\"\"Analyze goals with proper token settings\"\"\"\n",
    "        \n",
    "        text = scene_data.get('text', '')\n",
    "        if len(text) > self.max_goal_chunk_chars:\n",
    "            text = text[:self.max_goal_chunk_chars]  # Truncate if too long\n",
    "        \n",
    "        prompt = f'''Analyze character goals in this Baby-sitters Club scene:\n",
    "\n",
    "Scene: {scene_data.get('description', 'Scene from story')}\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "What do characters want or try to achieve? Respond in JSON:\n",
    "{{\n",
    "  \"goals\": [\n",
    "    {{\n",
    "      \"character\": \"Character Name\",\n",
    "      \"goal\": \"What they want to achieve\",\n",
    "      \"evidence\": \"Quote or description from text\",\n",
    "      \"category\": \"social/family/personal/academic/other\"\n",
    "    }}\n",
    "  ]\n",
    "}}'''\n",
    "        \n",
    "        try:\n",
    "            result = self.call_llm(prompt)\n",
    "            if result and 'goals' in result:\n",
    "                goals = result['goals']\n",
    "                # Add metadata\n",
    "                for goal in goals:\n",
    "                    goal['scene_id'] = scene_data.get('scene_id', 'unknown')\n",
    "                    goal['book_id'] = scene_data.get('book_id', 'unknown')\n",
    "                return goals\n",
    "        except Exception as e:\n",
    "            print(f\"         Error in goal analysis: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _split_text(self, text: str, max_chars: int):\n",
    "        \"\"\"Split text into chunks by paragraphs\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if len(current_chunk) + len(para) > max_chars and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = para\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + para if current_chunk else para\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print(\"‚úÖ WorkingStoryProcessor class defined with complete LLM integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# SIMPLE STORY PROCESSOR - ONE CLEAR SOLUTION\n",
    "# ====================================================================\n",
    "\n",
    "class SimpleStoryProcessor:\n",
    "    \"\"\"\n",
    "    A single, clear processor for story analysis.\n",
    "    No confusing multiple versions - just one that works!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = ollama_client\n",
    "        self.model = MODEL_NAME\n",
    "        \n",
    "    def analyze_story(self, story_text, story_id=\"story\"):\n",
    "        \"\"\"\n",
    "        Analyze a story and return scenes with goals.\n",
    "        \n",
    "        Args:\n",
    "            story_text: The text to analyze\n",
    "            story_id: ID for the story\n",
    "            \n",
    "        Returns:\n",
    "            List of Scene objects with goals\n",
    "        \"\"\"\n",
    "        print(f\"üìñ Analyzing story: {len(story_text):,} characters\")\n",
    "        \n",
    "        # Create prompt for scene segmentation\n",
    "        prompt = f'''Analyze this Baby-sitters Club story text and identify natural scene breaks.\n",
    "\n",
    "A scene is a continuous sequence in the same location/time. Look for:\n",
    "- Chapter breaks\n",
    "- Location changes  \n",
    "- Time jumps\n",
    "- Character perspective shifts\n",
    "\n",
    "Text to analyze:\n",
    "{story_text}\n",
    "\n",
    "Return JSON with this structure:\n",
    "{{\n",
    "  \"scenes\": [\n",
    "    {{\n",
    "      \"scene_id\": \"scene_1\",\n",
    "      \"description\": \"Brief description of what happens\",\n",
    "      \"text\": \"The actual scene text\"\n",
    "    }}\n",
    "  ]\n",
    "}}'''\n",
    "\n",
    "        try:\n",
    "            # Call LLM\n",
    "            print(\"üß† Calling LLM...\")\n",
    "            response = self.client.generate(\n",
    "                model=self.model,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'num_predict': 2000,  # Enough tokens for response\n",
    "                    'temperature': 0.3,   # Consistent results\n",
    "                    'num_ctx': 16000      # Good context size\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            response_text = response['response']\n",
    "            print(f\"‚úÖ Got response: {len(response_text)} characters\")\n",
    "            \n",
    "            # Parse JSON\n",
    "            import json\n",
    "            import re\n",
    "            \n",
    "            # Find JSON in response\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                scenes = []\n",
    "                for i, scene_data in enumerate(data.get('scenes', []), 1):\n",
    "                    scene = Scene(\n",
    "                        scene_id=f\"{story_id}_scene_{i}\",\n",
    "                        book_id=story_id,\n",
    "                        chapter_num=1,\n",
    "                        scene_num=i,\n",
    "                        text=scene_data.get('text', '')\n",
    "                    )\n",
    "                    scenes.append(scene)\n",
    "                \n",
    "                print(f\"üé¨ Created {len(scenes)} scenes\")\n",
    "                return scenes\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå No valid JSON found in response\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return []\n",
    "\n",
    "# Create the processor\n",
    "processor = SimpleStoryProcessor()\n",
    "print(\"‚úÖ Simple story processor created!\")\n",
    "print(\"   üìã Use: processor.analyze_story(text, 'story_id')\")\n",
    "print(\"   üéØ Returns: List of Scene objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# SIMPLE TEST - DOES IT WORK?\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üß™ TESTING THE SIMPLE PROCESSOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with sample text\n",
    "test_story = '''Chapter 1: The New Girl\n",
    "\n",
    "Shannon Kilbourne walked nervously into Stoneybrook Middle School. She had just moved from New York City, and everything felt different here.\n",
    "\n",
    "\"Hi there!\" called a friendly voice. A girl with brown hair approached her. \"I'm Mary Anne Spier. Are you new?\"\n",
    "\n",
    "\"Yes, I'm Shannon,\" she replied with relief. \"This place is so different from my old school.\"\n",
    "\n",
    "Chapter 2: Meeting the Club  \n",
    "\n",
    "At lunch, Mary Anne introduced Shannon to her friends.\n",
    "\n",
    "\"This is Kristy Thomas,\" Mary Anne said, pointing to a confident-looking girl. \"She started our babysitters club.\"\n",
    "\n",
    "\"Nice to meet you!\" Kristy said enthusiastically. \"We're always looking for good babysitters. Do you like kids?\"\n",
    "\n",
    "Shannon's eyes lit up. \"I love babysitting! Tell me more about this club.\"'''\n",
    "\n",
    "print(f\"üìù Test story: {len(test_story)} characters\")\n",
    "\n",
    "# Test the processor\n",
    "try:\n",
    "    scenes = processor.analyze_story(test_story, \"test_story\")\n",
    "    \n",
    "    if scenes:\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"‚úÖ Found {len(scenes)} scenes:\")\n",
    "        \n",
    "        for i, scene in enumerate(scenes, 1):\n",
    "            print(f\"\\n   Scene {i}: {scene.scene_id}\")\n",
    "            print(f\"   Preview: {scene.text[:80]}...\")\n",
    "            \n",
    "        # Store results for further use\n",
    "        story_scenes = {\"test_story\": scenes}\n",
    "        print(f\"\\nüìã Results stored in 'story_scenes' variable\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå No scenes found\")\n",
    "        story_scenes = {}\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test failed: {e}\")\n",
    "    story_scenes = {}\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CORPUS PROCESSING - RUN ON ENTIRE CORPUS\n",
    "# ====================================================================\n",
    "\n",
    "def process_entire_corpus():\n",
    "    \"\"\"Process all books in the corpus\"\"\"\n",
    "    print(\"üöÄ PROCESSING ENTIRE CORPUS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find all text files in the corpus\n",
    "    corpus_path = DATA_DIR / \"clean corpus no paratext\"\n",
    "    \n",
    "    if not corpus_path.exists():\n",
    "        print(f\"‚ùå Corpus directory not found: {corpus_path}\")\n",
    "        return {}\n",
    "    \n",
    "    txt_files = list(corpus_path.glob(\"*.txt\"))\n",
    "    print(f\"üìö Found {len(txt_files)} books to process\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for i, book_file in enumerate(txt_files, 1):\n",
    "        book_id = book_file.stem  # filename without extension\n",
    "        print(f\"\\nüìñ Processing {i}/{len(txt_files)}: {book_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the book\n",
    "            with open(book_file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            \n",
    "            print(f\"   üìù Text length: {len(text):,} characters\")\n",
    "            \n",
    "            # Limit text size for processing (adjust as needed)\n",
    "            if len(text) > 15000:\n",
    "                text = text[:15000]\n",
    "                print(f\"   ‚úÇÔ∏è Truncated to {len(text):,} characters\")\n",
    "            \n",
    "            # Process with our simple processor\n",
    "            scenes = processor.analyze_story(text, book_id)\n",
    "            \n",
    "            if scenes:\n",
    "                all_results[book_id] = {\n",
    "                    'scenes': scenes,\n",
    "                    'book_title': book_id.replace('_', ' ').title(),\n",
    "                    'scene_count': len(scenes)\n",
    "                }\n",
    "                print(f\"   ‚úÖ Success: {len(scenes)} scenes\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå No scenes found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {book_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüéä CORPUS PROCESSING COMPLETE!\")\n",
    "    print(f\"   üìä Successfully processed: {len(all_results)} books\")\n",
    "    print(f\"   üìã Total scenes: {sum(r['scene_count'] for r in all_results.values())}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Uncomment the line below to run corpus processing\n",
    "# corpus_results = process_entire_corpus()\n",
    "\n",
    "print(\"‚úÖ Corpus processing function ready!\")\n",
    "print(\"   üöÄ Run: corpus_results = process_entire_corpus()\")\n",
    "print(\"   ‚ö†Ô∏è  Note: This will take several minutes for the full corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HTML VISUALIZATION OUTPUT - FORMAT DATA FOR HTML FILE\n",
    "# ====================================================================\n",
    "\n",
    "def prepare_visualization_data(results_dict):\n",
    "    \"\"\"\n",
    "    Convert processing results to format needed for HTML visualization\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary from corpus processing\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary ready for JSON export to HTML file\n",
    "    \"\"\"\n",
    "    print(\"üé® PREPARING VISUALIZATION DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    visualization_data = {\n",
    "        \"metadata\": {\n",
    "            \"generated_date\": \"2025-08-11\",\n",
    "            \"total_books\": len(results_dict),\n",
    "            \"total_scenes\": sum(r['scene_count'] for r in results_dict.values()),\n",
    "            \"processor\": \"SimpleStoryProcessor\"\n",
    "        },\n",
    "        \"books\": []\n",
    "    }\n",
    "    \n",
    "    for book_id, book_data in results_dict.items():\n",
    "        book_viz = {\n",
    "            \"book_id\": book_id,\n",
    "            \"title\": book_data['book_title'],\n",
    "            \"scene_count\": book_data['scene_count'],\n",
    "            \"scenes\": []\n",
    "        }\n",
    "        \n",
    "        # Convert scenes to visualization format\n",
    "        for scene in book_data['scenes']:\n",
    "            scene_viz = {\n",
    "                \"scene_id\": scene.scene_id,\n",
    "                \"scene_num\": scene.scene_num,\n",
    "                \"text_preview\": scene.text[:200] + \"...\" if len(scene.text) > 200 else scene.text,\n",
    "                \"text_length\": len(scene.text),\n",
    "                \"chapter\": scene.chapter_num\n",
    "            }\n",
    "            book_viz[\"scenes\"].append(scene_viz)\n",
    "        \n",
    "        visualization_data[\"books\"].append(book_viz)\n",
    "    \n",
    "    print(f\"‚úÖ Prepared data for {len(results_dict)} books\")\n",
    "    return visualization_data\n",
    "\n",
    "def export_for_html_visualization(visualization_data, filename=\"scene_analysis_visualization.json\"):\n",
    "    \"\"\"Export data to JSON file for HTML visualization\"\"\"\n",
    "    import json\n",
    "    \n",
    "    output_file = Path(filename)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(visualization_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Exported to: {output_file}\")\n",
    "    print(f\"üìÅ File size: {output_file.stat().st_size:,} bytes\")\n",
    "    return output_file\n",
    "\n",
    "# Example usage function\n",
    "def create_html_visualization(results_dict):\n",
    "    \"\"\"Complete pipeline: results -> visualization data -> JSON file\"\"\"\n",
    "    print(\"üîÑ CREATING HTML VISUALIZATION DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    viz_data = prepare_visualization_data(results_dict)\n",
    "    \n",
    "    # Step 2: Export to JSON\n",
    "    json_file = export_for_html_visualization(viz_data)\n",
    "    \n",
    "    print(f\"\\nüéâ VISUALIZATION READY!\")\n",
    "    print(f\"   üìä Data: {viz_data['metadata']['total_books']} books, {viz_data['metadata']['total_scenes']} scenes\")\n",
    "    print(f\"   üìÅ File: {json_file}\")\n",
    "    print(f\"   üåê Ready for HTML dashboard!\")\n",
    "    \n",
    "    return viz_data, json_file\n",
    "\n",
    "print(\"‚úÖ HTML visualization functions ready!\")\n",
    "print(\"   üìä Use: viz_data, json_file = create_html_visualization(corpus_results)\")\n",
    "print(\"   üí° This will create the JSON file needed for the HTML dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMPLETE WORKFLOW EXAMPLE\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üìã COMPLETE WORKFLOW GUIDE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "üîÑ **STEP-BY-STEP WORKFLOW:**\n",
    "\n",
    "1Ô∏è‚É£ **Test with sample** (already done in previous cell):\n",
    "   ‚úÖ Small test to verify everything works\n",
    "\n",
    "2Ô∏è‚É£ **Process entire corpus**:\n",
    "   üìö corpus_results = process_entire_corpus()\n",
    "   ‚è±Ô∏è  Takes several minutes for full corpus\n",
    "\n",
    "3Ô∏è‚É£ **Create visualization data**:\n",
    "   üé® viz_data, json_file = create_html_visualization(corpus_results)\n",
    "   üìÅ Creates JSON file for HTML dashboard\n",
    "\n",
    "4Ô∏è‚É£ **Use the HTML file**:\n",
    "   üåê Open your HTML dashboard file\n",
    "   üìä It will load the JSON data automatically\n",
    "\n",
    "**Quick commands:**\n",
    "\"\"\")\n",
    "\n",
    "print(\"# For full corpus processing:\")\n",
    "print(\"corpus_results = process_entire_corpus()\")\n",
    "print()\n",
    "print(\"# For HTML visualization:\")\n",
    "print(\"viz_data, json_file = create_html_visualization(corpus_results)\")\n",
    "print()\n",
    "print(\"# Or do both in one go:\")\n",
    "print(\"corpus_results = process_entire_corpus()\")\n",
    "print(\"viz_data, json_file = create_html_visualization(corpus_results)\")\n",
    "\n",
    "print(\"\\nüí° **Tips:**\")\n",
    "print(\"   ‚Ä¢ Start with the test cell to make sure everything works\")\n",
    "print(\"   ‚Ä¢ Full corpus processing takes time - be patient!\")\n",
    "print(\"   ‚Ä¢ The JSON file will be created in the current directory\")\n",
    "print(\"   ‚Ä¢ Your HTML dashboard should automatically load the new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Features\n",
    "\n",
    "This comprehensive story analysis system provides:\n",
    "\n",
    "### ü§ñ **Multi-Provider LLM Support**\n",
    "- **Anthropic Claude**: Production-ready with large context (200k tokens)\n",
    "- **OpenAI GPT**: Industry standard with excellent performance (128k tokens)  \n",
    "- **Ollama Local**: Privacy-focused local processing (configurable models)\n",
    "- **Easy Switching**: Interactive widgets for seamless provider changes\n",
    "- **Unified Interface**: Same API calls work across all providers\n",
    "\n",
    "### üîÑ **Multi-pass LLM Processing**\n",
    "- **Pass 1**: Scene segmentation using LLM to identify natural scene breaks\n",
    "- **Pass 2**: Narrator identification at the chapter level\n",
    "- **Pass 3**: Goal analysis with dual categorization and evidence\n",
    "\n",
    "### üìä **Scalable Processing**\n",
    "- Run on sample data for testing or full corpus for production\n",
    "- Incremental progress tracking with resume capability\n",
    "- Real-time progress reports during processing\n",
    "- Robust error handling and retry logic\n",
    "\n",
    "### üéõÔ∏è **Interactive GUI**\n",
    "- Browse books, chapters, and scenes\n",
    "- View scene text and associated goals\n",
    "- Edit and save changes to analysis results\n",
    "- Export data for visualization\n",
    "\n",
    "### üéØ **Enhanced Goal Analysis**\n",
    "- **Dual categorization**: Internal/External + Academic/Family/Personal/etc.\n",
    "- **Evidence-based**: LLM provides text evidence for each goal\n",
    "- **Chapter-aware scene IDs**: Retains chapter membership\n",
    "- **Confidence scoring**: LLM confidence for each analysis\n",
    "\n",
    "### üìà **Data Export & Visualization**\n",
    "- JSON export optimized for website visualization\n",
    "- Summary statistics and cross-tabulations\n",
    "- Individual book files for lazy loading\n",
    "- Compressed formats for web deployment\n",
    "\n",
    "### üîß **Technical Features**\n",
    "- Multiple LLM provider support with unified interface\n",
    "- Provider-specific optimizations (token limits, context sizes)\n",
    "- API key management and connection testing\n",
    "- Multiple narrator support per book\n",
    "- Automatic progress saving and resumption\n",
    "- Clean, modular architecture\n",
    "\n",
    "### üöÄ **Getting Started**\n",
    "1. **Choose Provider**: Use the configuration widget to select Anthropic, OpenAI, or Ollama\n",
    "2. **Set Credentials**: Configure API keys or local Ollama setup\n",
    "3. **Test Connection**: Verify your provider is working\n",
    "4. **Run Analysis**: Process your corpus with `processor.process_all()`\n",
    "5. **Review Results**: Use the interactive GUI to browse and edit findings\n",
    "\n",
    "### üí° **Provider Recommendations**\n",
    "- **Anthropic Claude**: Best for large books, complex analysis, production use\n",
    "- **OpenAI GPT**: Excellent balance of performance and cost\n",
    "- **Ollama Local**: Perfect for privacy, experimentation, and cost-free processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a set of files from Project Gutenberg via the API\n",
    "import requests\n",
    "from pathlib import Path\n",
    "def download_gutenberg_books(book_ids, save_dir=\"gutenberg_books\"):\n",
    "    \"\"\"Download plain text files for a list of Gutenberg book IDs.\"\"\"\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    for book_id in book_ids:\n",
    "        url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(f\"{save_dir}/{book_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"‚úÖ Downloaded book {book_id}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to download book {book_id} (status {response.status_code})\")\n",
    "# Example usage: download_gutenberg_books([1342, 1661, 2701])  # Pride and Prejudice, Sherlock Holmes, Moby Dick"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
